{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "shared-award",
   "metadata": {},
   "source": [
    "## Reading in and cleaning the data\n",
    "Reading in the data for both the test set and the training set I've decided to remove stopwords before we get any counts for any words at all. Removing these words will help our predicting performance as these stop words aren't actually really related to the class, for example the which is the most common word in the english language will not have any correlation to what the actual text is about and its class. Hence removing these words will improve our model performance and not try to make predictions on attributes which are uncorrelated/unimportant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-accuracy",
   "metadata": {},
   "source": [
    "### The data\n",
    "the data we read in had a huge class imbalance where we only had ~130 class instances for A and V while B & E had ~1600 and ~1300 instances respectively. This meant that the priors for both A and V would be massively overshadowed by the priors to B & E which is one of the major problems of Naive Bayes if our training set is not very representative of real world data. Ie if we had in the real world, more classifications of A & V then our naive bayes would work terribly. To tackle this huge class imbalance there is such a thing as Complement Naive Bayes which is one of the implementations I decided to use to increase the prediction performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-acrylic",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "For calculating the accuracy I just created a simple function called accuracy_metric. This function will calculate the accuracy and the performance of the naive bayes models predictions by comparing the actual values from the test set to the predicted classes from our Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-windows",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "To validate the performance of the model I chose to use cross validation with the accruacy metric to run the model on different training and test sets. I chose to run the algorithms on a 10-fold cross validation 10 times over. I wanted run the 10-fold cross validation more times like 30 to really get an accurate representation of the different extended Naive Bayes implementations. Also I found that using stratified cross validation would've been a much better way to get an accurate representation of our model performance due to the huge class imbalance. But I ran out of time for implementation so I cannot be 100% accurate with the model performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-brunei",
   "metadata": {},
   "source": [
    "### Training the model and getting prior probabilities\n",
    "To \"train\" our naive bayes model I decided to go through each document and count each occurence of a word in that document using the Python Counter built in module. The counts would be attributed to a dict of words with their counts to each different class in the whole dataset. Hence, we get the occurence of words given a class. After getting these counts we get the priori through simply doing a division of the count of a class over the total amount of documents we read over. Hence giving us prior probabilites of 0.032,0.4005,0.0315,0.536 to a,b,v,e respectively which again shows us the imbalance talked about before. So using these priors we will have a prediction that it is a class given a document, It will in this case most likely be B and E."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-scroll",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes\n",
    "For implementing the standard Naive Bayes I chose to first have a training function where we get all the probabilites of a word occuring in that class. For example all the probabilities for class a wouled be represneted as a_probabilites={'The':1,.... }.\n",
    "I got these probabilites by going through the training data, calculating the frequency of a word in a class lets say Nc, getting the sum of all unique words we've trained on so far |V| , getting a count of all words in a specific class,Nci, and then using these variables to get the probabilites.\n",
    "I iterated through each word in a document and assigning the probability of a class given that word for each class by doing the following.\n",
    "\n",
    "    Iterate through each document\n",
    "        Count each word in document\n",
    "                calculate Nc + 1 / Nci + |V| for each class and each word\n",
    "            apply the priori to above claculation to specific class\n",
    "        return all probabilites for a class of each word\n",
    "    \n",
    "The calculation I used was with laplace smoothing to ensure that if a word that hasnt been seen before we would not get a 0 division error.\n",
    "Then on the testing it would be pretty similar\n",
    "\n",
    "    Itrate through each testing document\n",
    "        count each word in document\n",
    "                calculate Nc + 1 / Nci + |V| for each class and each word if not seen already\n",
    "                times that calculation by the frequency of word in this document.\n",
    "            apply the priori\n",
    "            return all probabilites for a class of each word\n",
    "        return the probability of a class of that specific document\n",
    "    return all predictions of a class given the document\n",
    "\n",
    "The performance I recieved for standarnd multinomial Naive Bayes was pretty bad, I got an accuracy of around 50% so it wasn't very good at predicting much at all, it would just choose the majority class which was almost always E and this could be due to the prior of E which was quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-paste",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes\n",
    "For my extension of Naive Bayes I decided to add TF * IDF to discount words which would appear very frequently in the documents. If a word would appear in many documents in different classes then it won't really help us in our classification as that word most likely isn't related to the class and is hence pretty useless. With TF * IDF we apply a weighting to our words to prefer the rarer words and make their probability for a class higher than common words which will apear in every text, the texts with high frequency will recieve a weighting which reduces the probability of that word given a class which has significantly increased the performance of the algorithm. This gave an accuracy of about 91.7 when run through cross validation, I thought that I could get a higher accuracy than this so I decided to combine this with Complement Naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-incentive",
   "metadata": {},
   "source": [
    "### Complement TF * IDF Naive Bayes\n",
    "In complement Naive bayes we instead of calculating the likelihood that a document is inside a class by calculating the occurance of that word in a class / all words in that class (and laplace smoothing) we instead calculate the likelihood by calculating the occurance of word not in class c / all words not in class c. This will reduce the load of the huge class imbalance I've mentioned earlier by negating the priors for each class as well, when ran with TF&IDF we got a increase in performance of around 1% but due to not being able to find a suitable way to calculate the statistical significance of the means of these classes I cannot be 100% to say this was not attributed to chance rather than an actual increase in accuracy. But testing as well on the kaggle performance, this gave a much higher accuracy than just standard TF&IDF Naive Bayes which was around 92% acuracy while the accuracy for Complement TF * IDF Naive Bayes was 93.36% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-behavior",
   "metadata": {},
   "source": [
    "#### N grams\n",
    "I as well implemented N grams to hopefully increase the performance of the model but due to the size of the dataset this was quite unlikely as we would need a much larger dataset to find common occurence of lets say \"a virus\" rather than just \"a\" and \"virus\". N grams would ensure that we wouldn't lose the meaning of a sentence so we would include lets say 2 words instead of them single-handedly but we would need a much larger dataset to represent all different N-grams. So when ran with cross validation the performance of the model actually decreased due to this, hence I did not include it as my extension but more so added it in to talk about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "entitled-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import numpy\n",
    "import math\n",
    "import csv\n",
    "import operator\n",
    "from random import seed\n",
    "from random import randrange\n",
    "import multiprocessing\n",
    "from random import randrange\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "valid-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_words = [\"the\", \"of\", \"and\", \"a\", \"in\", \"to\", \"that\", \"is\", \"with\", \"for\", \"from\", \"are\", \"by\", \" \", \"was\", \"we\", \"this\", \"were\", \"as\", \"an\", \"have\" ,\"which\", \"has\", \"these\", \"at\", \"be\"]\n",
    "with open('trg.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    for i in data:\n",
    "        a_str = \"\"\n",
    "        for j in i[2].split():\n",
    "            if j not in redundant_words:\n",
    "                a_str += j + \" \"\n",
    "        i[2] = a_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "allied-plate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'abstract ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tst.csv', newline='') as csvfile:\n",
    "    test_data = list(csv.reader(csvfile))\n",
    "    for i in test_data:\n",
    "        a_str = \"\"\n",
    "        for j in i[1].split():\n",
    "            if j not in redundant_words:\n",
    "                a_str += j + \" \"\n",
    "        i[1] = a_str\n",
    "test_data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "jewish-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i][1] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cardiovascular-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "def cross_validation_split(dataset, folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "logical-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "a_counter = Counter()\n",
    "e_counter = Counter()\n",
    "b_counter = Counter()\n",
    "v_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "egyptian-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'E': 2144, 'B': 1602, 'A': 128, 'V': 126})\n",
      "32216\n"
     ]
    }
   ],
   "source": [
    "def get_counts(data):\n",
    "    text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "    a_counter = Counter()\n",
    "    e_counter = Counter()\n",
    "    b_counter = Counter()\n",
    "    v_counter = Counter()\n",
    "    word_freq = Counter()\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if count != 0:\n",
    "            x.append(i[2])\n",
    "            y.append(i[1])\n",
    "            if (i[1] == \"A\"):\n",
    "                a = Counter(i[2].split())\n",
    "                a_counter += a\n",
    "                word_freq += Counter(set(a))\n",
    "            if (i[1] == \"B\"):\n",
    "                b = Counter(i[2].split())\n",
    "                b_counter += b\n",
    "                word_freq += Counter(set(b))\n",
    "            if (i[1] == \"E\"):\n",
    "                e = Counter(i[2].split())\n",
    "                e_counter += e\n",
    "                word_freq += Counter(set(e))\n",
    "            if (i[1] == \"V\"):\n",
    "                v = Counter(i[2].split())\n",
    "                v_counter += v\n",
    "                word_freq += Counter(set(v))\n",
    "        count += 1\n",
    "    target_counter = Counter(y)\n",
    "    \n",
    "    return word_freq, target_counter, a_counter, b_counter, e_counter, v_counter\n",
    "\n",
    "word_freq_in_doc, target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(data)\n",
    "##print(a_counter)\n",
    "print(target_counter)\n",
    "print(len(word_freq_in_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "opposed-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum = sum(target_counter.values())\n",
    "a_probability = target_counter[\"A\"] / a_sum\n",
    "b_probability = target_counter[\"B\"] / a_sum\n",
    "v_probability = target_counter[\"V\"] / a_sum\n",
    "e_probability = target_counter[\"E\"] / a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "turkish-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032\n",
      "0.4005\n",
      "0.0315\n",
      "0.536\n"
     ]
    }
   ],
   "source": [
    "print(a_probability)\n",
    "print(b_probability)\n",
    "print(v_probability)\n",
    "print(e_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-kitchen",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "municipal-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_probabilites = {}\n",
    "b_probabilites = {}\n",
    "v_probabilites = {}\n",
    "e_probabilites = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-latitude",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "featured-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_word_sum = sum(a_counter.values())\n",
    "b_word_sum = sum(b_counter.values())\n",
    "v_word_sum = sum(v_counter.values())\n",
    "e_word_sum = sum(e_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adequate-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "overhead-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(all_unique, a_counter, b_counter, e_counter, v_counter):\n",
    "    a_probabilites = {}\n",
    "    b_probabilites = {}\n",
    "    v_probabilites = {}\n",
    "    e_probabilites = {}\n",
    "    for word in all_unique:\n",
    "        if word not in a_counter.keys():\n",
    "            a_prob = 1\n",
    "        elif word in a_counter.keys():\n",
    "            a_prob = a_counter.get(word) + 1\n",
    "        a_probabilites[word] = a_prob / a_word_sum + sum_unique\n",
    "\n",
    "        if word not in b_counter.keys():\n",
    "            b_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            b_prob = b_counter.get(word) + 1\n",
    "        b_probabilites[word] = b_prob / b_word_sum + sum_unique\n",
    "\n",
    "        if word not in v_counter.keys():\n",
    "            v_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            v_prob = v_counter.get(word) + 1\n",
    "        v_probabilites[word] = v_prob / v_word_sum + sum_unique\n",
    "\n",
    "\n",
    "        if word not in e_counter.keys():\n",
    "            e_prob = 1\n",
    "        elif word in e_counter.keys():\n",
    "            e_prob = e_counter.get(word) + 1\n",
    "        e_probabilites[word] = e_prob / e_word_sum + sum_unique\n",
    "    return a_probabilites, b_probabilites, v_probabilites, e_probabilites\n",
    "a_probabilites, b_probabilites, v_probabilites, e_probabilites = standard(all_unique, a_counter, b_counter, e_counter, v_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-brake",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_test(all_unique,test_data,  a_counter, b_counter, e_counter, v_counter, index, a_probabilites, b_probabilites, v_probabilites, e_probabilites):\n",
    "    all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "    sum_unique = len(all_unique.keys())\n",
    "    a_word_sum = len(a_counter.values())\n",
    "    b_word_sum = len(b_counter.values())\n",
    "    v_word_sum = len(v_counter.values())\n",
    "    e_word_sum = len(e_counter.values())\n",
    "    predictions = []\n",
    "\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "        for word in i[index].split():\n",
    "            a_prob = a_probabilites.get(word)\n",
    "            if (a_prob == None):\n",
    "                a_prob = 1 / a_word_sum + sum_unique\n",
    "            class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "\n",
    "            b_prob = b_probabilites.get(word) \n",
    "            if (b_prob == None):\n",
    "                b_prob = 1 / b_word_sum + sum_unique\n",
    "            class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            v_prob = v_probabilites.get(word)\n",
    "            if (v_prob == None):\n",
    "                v_prob = 1 / v_word_sum + sum_unique\n",
    "            class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "\n",
    "            e_prob = e_probabilites.get(word)\n",
    "            if (e_prob == None):\n",
    "                e_prob = 1 / e_word_sum + sum_unique\n",
    "            class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        class_prob[\"A\"] = class_prob[\"A\"] + math.log(a_probability)\n",
    "        class_prob[\"B\"] = class_prob[\"B\"] + math.log(b_probability)\n",
    "        class_prob[\"V\"] = class_prob[\"V\"] + math.log(v_probability)\n",
    "        class_prob[\"E\"] = class_prob[\"E\"] + math.log(e_probability)\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    return predictions\n",
    "predictions = standard_test(all_unique,test_data, a_counter, b_counter, e_counter, v_counter, 1, a_probabilites, b_probabilites, v_probabilites, e_probabilites)\n",
    "##print(all_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "rand_split = cross_validation_split(data, k)\n",
    "overall_accuracies = []\n",
    "for i in range(10):\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        test_split = rand_split[i]\n",
    "        training_split = rand_split.copy()\n",
    "        training_split.pop(i)\n",
    "\n",
    "        training_split = [item for sublist in training_split for item in sublist]\n",
    "        text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "        a_counter = Counter()\n",
    "        e_counter = Counter()\n",
    "        b_counter = Counter()\n",
    "        v_counter = Counter()\n",
    "\n",
    "        word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "    #    a_counter = Counter(dict(a_counter.most_common(1250)))\n",
    "    #    b_counter = Counter(dict(b_counter.most_common(1250)))\n",
    "    #    e_counter = Counter(dict(e_counter.most_common(1250)))\n",
    "    #    v_counter = Counter(dict(v_counter.most_common(1250)))\n",
    "\n",
    "        all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "        sum_unique = len(all_unique.keys())\n",
    "\n",
    "        a_sum = sum(target_counter.values())\n",
    "        a_probability = target_counter[\"A\"] / a_sum\n",
    "        b_probability = target_counter[\"B\"] / a_sum\n",
    "        v_probability = target_counter[\"V\"] / a_sum\n",
    "        e_probability = target_counter[\"E\"] / a_sum\n",
    "\n",
    "\n",
    "        a_sum = len(a_counter.values())\n",
    "        b_sum = len(b_counter.values())\n",
    "        v_sum = len(v_counter.values())\n",
    "        e_sum = len(e_counter.values())\n",
    "\n",
    "        a_probabilites, b_probabilites, v_probabilites, e_probabilites = standard(all_unique, a_counter, b_counter, e_counter, v_counter)\n",
    "        predictions = standard_test(all_unique,test_split,  a_counter, b_counter, e_counter, v_counter, 2, a_probabilites, b_probabilites, v_probabilites, e_probabilites)\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy_metric(test_split, predictions))\n",
    "        print(accuracies)\n",
    "    overall_accuracies.append(statistics.mean(accuracies))\n",
    "    #print(overall_accuracies)\n",
    "print(overall_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-convert",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_train(training_split_len, all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, a_counter, b_counter, e_counter, v_counter):\n",
    "    a_probabilites = {}\n",
    "    b_probabilites = {}\n",
    "    v_probabilites = {}\n",
    "    e_probabilites = {}\n",
    "    \n",
    "    for word in all_unique:\n",
    "\n",
    "        idf = math.log(training_split_len / word_freq_in_doc.get(word))\n",
    "\n",
    "        if word not in a_counter.keys():\n",
    "            a_prob = 1\n",
    "        elif word in a_counter.keys():\n",
    "            a_prob = a_counter.get(word) + 1\n",
    "        a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "        if a_prob == 0 or a_prob < 0:\n",
    "            a_prob = 0.00001\n",
    "        a_probabilites[word] = a_prob \n",
    "\n",
    "        if word not in b_counter.keys():\n",
    "            b_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            b_prob = b_counter.get(word) + 1\n",
    "        b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "        if b_prob == 0 or b_prob < 0:\n",
    "            b_prob = 0.00001\n",
    "        b_probabilites[word] = b_prob\n",
    "\n",
    "        if word not in v_counter.keys():\n",
    "            v_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            v_prob = v_counter.get(word) + 1\n",
    "        v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "        if v_prob == 0 or v_prob < 0:\n",
    "            v_prob = 0.00001\n",
    "        v_probabilites[word] = v_prob\n",
    "\n",
    "\n",
    "        if word not in e_counter.keys():\n",
    "            e_prob = 1\n",
    "        elif word in e_counter.keys():\n",
    "            e_prob = e_counter.get(word) + 1\n",
    "        e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "        if e_prob == 0 or e_prob < 0:\n",
    "            e_prob = 0.00001\n",
    "        e_probabilites[word] = e_prob\n",
    "        \n",
    "    return a_probabilites, b_probabilites, e_probabilites, v_probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_probabilites, b_probabilites, e_probabilites, v_probabilites = tf_idf_train(len(data),all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique,  a_counter, b_counter, e_counter, v_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-license",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_test(a_word_sum, b_word_sum, v_word_sum, e_word_sum,test_data, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, index):\n",
    "    predictions = []\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "        for word in i[index].split():\n",
    "            a_prob = a_probabilites.get(word)\n",
    "            if (a_prob == None):\n",
    "                a_prob = 1 / a_word_sum + sum_unique\n",
    "            class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "\n",
    "            b_prob = b_probabilites.get(word) \n",
    "            if (b_prob == None):\n",
    "                b_prob = 1 / b_word_sum + sum_unique\n",
    "            class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            v_prob = v_probabilites.get(word)\n",
    "            if (v_prob == None):\n",
    "                v_prob = 1 / v_word_sum + sum_unique\n",
    "            class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "\n",
    "            e_prob = e_probabilites.get(word)\n",
    "            if (e_prob == None):\n",
    "                e_prob = 1 / e_word_sum + sum_unique\n",
    "            class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        class_prob[\"A\"] =  math.log(a_probability) + class_prob[\"A\"]\n",
    "        class_prob[\"B\"] =  math.log(b_probability) + class_prob[\"B\"]\n",
    "        class_prob[\"V\"] =  math.log(v_probability) + class_prob[\"V\"]\n",
    "        class_prob[\"E\"] =  math.log(e_probability) + class_prob[\"E\"]\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    return predictions\n",
    "predictions = tf_idf_test(a_word_sum, b_word_sum, v_word_sum, e_word_sum,test_data, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 1)\n",
    "##print(all_unique)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-trust",
   "metadata": {},
   "source": [
    "### Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "rand_split = cross_validation_split(data, k)\n",
    "overall_accuracies = []\n",
    "for i in range(10):\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        test_split = rand_split[i]\n",
    "        training_split = rand_split.copy()\n",
    "        training_split.pop(i)\n",
    "\n",
    "        training_split = [item for sublist in training_split for item in sublist]\n",
    "        text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "        a_counter = Counter()\n",
    "        e_counter = Counter()\n",
    "        b_counter = Counter()\n",
    "        v_counter = Counter()\n",
    "\n",
    "        word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "    #    a_counter = Counter(dict(a_counter.most_common(1250)))\n",
    "    #    b_counter = Counter(dict(b_counter.most_common(1250)))\n",
    "    #    e_counter = Counter(dict(e_counter.most_common(1250)))\n",
    "    #    v_counter = Counter(dict(v_counter.most_common(1250)))\n",
    "\n",
    "        all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "        sum_unique = len(all_unique.keys())\n",
    "\n",
    "        a_sum = sum(target_counter.values())\n",
    "        a_probability = target_counter[\"A\"] / a_sum\n",
    "        b_probability = target_counter[\"B\"] / a_sum\n",
    "        v_probability = target_counter[\"V\"] / a_sum\n",
    "        e_probability = target_counter[\"E\"] / a_sum\n",
    "\n",
    "\n",
    "        a_sum = len(a_counter.values())\n",
    "        b_sum = len(b_counter.values())\n",
    "        v_sum = len(v_counter.values())\n",
    "        e_sum = len(e_counter.values())\n",
    "\n",
    "        a_probabilites, b_probabilites, e_probabilites, v_probabilites = tf_idf_train(len(training_split), all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, a_counter, b_counter, e_counter, v_counter)\n",
    "        predictions = tf_idf_test(a_sum, b_sum, v_sum, e_sum,test_split, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 2)\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy_metric(test_split, predictions))\n",
    "        print(accuracies)\n",
    "    overall_accuracies.append(statistics.mean(accuracies))\n",
    "    #print(overall_accuracies)\n",
    "print(overall_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(overall_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-sensitivity",
   "metadata": {},
   "source": [
    "### Accuracy with TF*IDF\n",
    "after a 10-fold cross validation which was run 10 times I got an average accuracy of 91.704166666667 which shows us that our model performed much better after applying a wieghting to the frequencies of the words overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-facing",
   "metadata": {},
   "source": [
    "### Complement TF*IDF Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_idf(a_counter, b_counter, v_counter, e_counter, a_probability, b_probability, v_probability, e_probability, test_data, training_len, index):\n",
    "    predictions = []\n",
    "    # Complement naive bayes so get all words in other classes\n",
    "    not_a = e_counter + b_counter + v_counter\n",
    "    not_b = a_counter + e_counter + v_counter\n",
    "    not_e = a_counter + v_counter + b_counter\n",
    "    not_v = a_counter + e_counter + b_counter\n",
    "    \n",
    "    sum_unique = a_counter + b_counter + v_counter + e_counter\n",
    "    sum_unique = len(sum_unique.keys())\n",
    "\n",
    "    #Get the sum of all counts in a class. Total word count for each class\n",
    "    a_sum = sum(not_a.values())\n",
    "    b_sum = sum(not_b.values())\n",
    "    e_sum = sum(not_e.values())\n",
    "    v_sum = sum(not_v.values())\n",
    "\n",
    "    #Iterate through the csv\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        # For every document, do a word count\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 1, \"B\": 1, \"V\": 1, \"E\": 1}\n",
    "        # Iterate through each word in a document\n",
    "        for word in i[index].split():\n",
    "            # Get the frequency of a word in all training documents\n",
    "            if word in word_freq_in_doc:\n",
    "                word_in_doc = word_freq_in_doc[word]\n",
    "            else:\n",
    "                word_in_doc = 0\n",
    "    #        print(word_in_doc)\n",
    "            #Calculate the idf \n",
    "            if word_in_doc != 0:\n",
    "                idf = math.log(training_len / word_in_doc)\n",
    "            else:\n",
    "                idf = 0\n",
    "            if word not in not_a.keys():\n",
    "                a_prob = 1\n",
    "            elif word in not_a.keys():\n",
    "                a_prob = not_a.get(word) + 1\n",
    "            a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "            if a_prob == 0 or a_prob < 0:\n",
    "                a_prob = 0.00001\n",
    "           # print(a_prob)\n",
    "            class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "            if word not in not_b.keys():\n",
    "                b_prob = 1\n",
    "            elif word in not_b.keys():\n",
    "                b_prob = not_b.get(word) + 1\n",
    "           # print(\"b_Probability \" + str(b_prob))\n",
    "            #print(\"idf \" + str(idf))\n",
    "            b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "            if b_prob == 0 or b_prob < 0:\n",
    "                b_prob = 0.00001\n",
    "            class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            if word not in not_v.keys():\n",
    "                v_prob = 1\n",
    "            elif word in not_v.keys():\n",
    "                v_prob = not_v.get(word) + 1\n",
    "            v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "            if v_prob == 0 or v_prob <0:\n",
    "                v_prob = 0.00001\n",
    "            class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "            if word not in not_e.keys():\n",
    "                e_prob = 1\n",
    "            elif word in not_e.keys():\n",
    "                e_prob = not_e.get(word) + 1\n",
    "            e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "            if e_prob == 0 or e_prob < 0:\n",
    "                e_prob = 0.00001\n",
    "            class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        class_prob[\"A\"] =  math.log(a_probability) - class_prob[\"A\"]\n",
    "        class_prob[\"B\"] =  math.log(b_probability) - class_prob[\"B\"]\n",
    "        class_prob[\"V\"] =  math.log(v_probability) - class_prob[\"V\"]\n",
    "        class_prob[\"E\"] =  math.log(e_probability) - class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    return predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy = []\n",
    "for j in range(10):\n",
    "    k = 10\n",
    "    rand_split = cross_validation_split(data, k)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        test_split = rand_split[i]\n",
    "        training_split = rand_split.copy()\n",
    "        training_split.pop(i)\n",
    "\n",
    "        training_split = [item for sublist in training_split for item in sublist]\n",
    "        text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "        a_counter = Counter()\n",
    "        e_counter = Counter()\n",
    "        b_counter = Counter()\n",
    "        v_counter = Counter()\n",
    "\n",
    "        word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "        a_sum = sum(target_counter.values())\n",
    "        a_probability = target_counter[\"A\"] / a_sum\n",
    "        b_probability = target_counter[\"B\"] / a_sum\n",
    "        v_probability = target_counter[\"V\"] / a_sum\n",
    "        e_probability = target_counter[\"E\"] / a_sum\n",
    "\n",
    "        training_len = len(training_split)\n",
    "\n",
    "        predictions = complement_idf(a_counter, b_counter, v_counter, e_counter, a_probability, b_probability, v_probability, e_probability, test_split, training_len, 2)\n",
    "\n",
    "\n",
    "    #    a_probabilites, b_probabilites, e_probabilites, v_probabilites = complement_train(len(training_split), all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, not_a, not_b, not_e, not_v)\n",
    "     #   predictions = complement_test(word_freq_in_doc,a_sum, b_sum, v_sum, e_sum,len(training_split),test_split, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 2)\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy_metric(test_split, predictions))\n",
    "    print(accuracies)\n",
    "    overall_accuracy.append(statistics.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(overall_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"class\"])\n",
    "    for i in range(len(predictions)):\n",
    "        writer.writerow([i+1, predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram(s, n):\n",
    "    \n",
    "    ngrams = [ngram for ngram in s.split(\" \") if ngram != \"\"]\n",
    "    \n",
    "    ngrams = zip(*[ngrams[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
