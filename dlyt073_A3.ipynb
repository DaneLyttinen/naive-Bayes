{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entitled-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import numpy\n",
    "import math\n",
    "import csv\n",
    "import operator\n",
    "from random import seed\n",
    "from random import randrange\n",
    "import multiprocessing\n",
    "from random import randrange\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valid-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'B', '4 202 353 bp genome alkaliphilic bacterium bacillus halodurans c-125 contains 4066 predicted protein coding sequences cdss 2141 527 functional assignments 1182 29 conserved cdss unknown function 743 18 3 no match any protein database among total cdss 88 match sequences proteins found only bacillus subtilis 667 widely conserved comparison proteins various organisms including bsubtilis b halodurans genome contains 112 transposase genes indicating transposases played important evolutionary role horizontal gene transfer also internal genetic rearrangement genome strain c-125 lacks some necessary genes competence such coms srfa rapc supporting fact competence not been demonstrated experimentally c-125 there no paralog tupa encoding teichuronopeptide contributes alkaliphily c-125 genome ortholog tupa cannot found bsubtilis genome out 11 sigma factors belong extracytoplasmic function family 10 unique b halodurans suggesting they may role special mechanism adaptation alkaline environment ']\n"
     ]
    }
   ],
   "source": [
    "redundant_words = [\"the\", \"of\", \"and\", \"a\", \"in\", \"to\", \"that\", \"is\", \"with\", \"for\", \"from\", \"are\", \"by\", \" \", \"was\", \"we\", \"this\", \"were\", \"as\", \"an\", \"have\" ,\"which\", \"has\", \"these\", \"at\", \"be\"]\n",
    "with open('trg.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    for i in data:\n",
    "        a_str = \"\"\n",
    "        for j in i[2].split():\n",
    "            if j not in redundant_words:\n",
    "                a_str += j + \" \"\n",
    "        i[2] = a_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "allied-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'previous work all three components comamonas testosteroni b-356 biphenyl bphchlorobiphenyls pcbs dioxygenase dox been purified characterized they include iron-sulphur protein ispbph terminal oxygenase composed two subunits encoded bpha bphe ferredoxin ferbph encoded bphf reductase redbph encoded bphg bphg not located neighbourhood bphaef b-356 reporting cloning b-356-bphg sequencing b-356-bph dox genes comparative analysis genes provided genetic evidence showing two bph dox lineages emerged gram-negative bacteria main features lineage includes b-356 location bphg outside bph gene cluster structure redbph very distinct all other aryl dioxygenase-reductases ']\n"
     ]
    }
   ],
   "source": [
    "with open('tst.csv', newline='') as csvfile:\n",
    "    test_data = list(csv.reader(csvfile))\n",
    "    for i in test_data:\n",
    "        a_str = \"\"\n",
    "        for j in i[1].split():\n",
    "            if j not in redundant_words:\n",
    "                a_str += j + \" \"\n",
    "        i[1] = a_str\n",
    "test_data.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-award",
   "metadata": {},
   "source": [
    "### Reading in and cleaning the data\n",
    "Reading in the data for both the test set and the training set I've decided to remove stopwords before we get any counts for any words at all. Removing these words will help our predicting performance as these stop words aren't actually really related to the class, for example the which is the most common word in the english language will not have any correlation to what the actual text is about. Hence removing these words will improve our model performance and not try to make predictions on attributes which are uncorrelated/unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jewish-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i][1] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-stadium",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Here we are simply creating a accuracy calculating function for our cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cardiovascular-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "def cross_validation_split(dataset, folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-college",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "To create a cross validation split we need to first split the dataset into k equal parts, which we do above and then use 1 of these as a test set and the rest as the training set while changing the test set to another after we've iterated after each test set so we can get an accurate representation of our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "logical-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "a_counter = Counter()\n",
    "e_counter = Counter()\n",
    "b_counter = Counter()\n",
    "v_counter = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-client",
   "metadata": {},
   "source": [
    "### Training the model and getting prior probabilities\n",
    "This is a function where we get a dataset and then calculate each word count for each class and how many times a class will appear overall in our training dataset. Through these word counts we can start calculating probabilites with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "egyptian-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'E': 2144, 'B': 1602, 'A': 128, 'V': 126})\n",
      "32216\n"
     ]
    }
   ],
   "source": [
    "def get_counts(data):\n",
    "    text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "    a_counter = Counter()\n",
    "    e_counter = Counter()\n",
    "    b_counter = Counter()\n",
    "    v_counter = Counter()\n",
    "    word_freq = Counter()\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if count != 0:\n",
    "            x.append(i[2])\n",
    "            y.append(i[1])\n",
    "            if (i[1] == \"A\"):\n",
    "                a = Counter(i[2].split())\n",
    "                a_counter += a\n",
    "                word_freq += Counter(set(a))\n",
    "            if (i[1] == \"B\"):\n",
    "                b = Counter(i[2].split())\n",
    "                b_counter += b\n",
    "                word_freq += Counter(set(b))\n",
    "            if (i[1] == \"E\"):\n",
    "                e = Counter(i[2].split())\n",
    "                e_counter += e\n",
    "                word_freq += Counter(set(e))\n",
    "            if (i[1] == \"V\"):\n",
    "                v = Counter(i[2].split())\n",
    "                v_counter += v\n",
    "                word_freq += Counter(set(v))\n",
    "        count += 1\n",
    "    target_counter = Counter(y)\n",
    "    \n",
    "    return word_freq, target_counter, a_counter, b_counter, e_counter, v_counter\n",
    "\n",
    "word_freq_in_doc, target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(data)\n",
    "##print(a_counter)\n",
    "print(target_counter)\n",
    "print(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opposed-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum = sum(target_counter.values())\n",
    "a_probability = target_counter[\"A\"] / a_sum\n",
    "b_probability = target_counter[\"B\"] / a_sum\n",
    "v_probability = target_counter[\"V\"] / a_sum\n",
    "e_probability = target_counter[\"E\"] / a_sum\n",
    "all_unique = a_counter + b_counter + e_counter + v_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turkish-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032\n",
      "0.4005\n",
      "0.0315\n",
      "0.536\n"
     ]
    }
   ],
   "source": [
    "print(a_probability)\n",
    "print(b_probability)\n",
    "print(v_probability)\n",
    "print(e_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-kitchen",
   "metadata": {},
   "source": [
    "#### Priori\n",
    "As we can see above we have very unbalanced priori between the classes which will affect the predicted probabilites of our naive bayes. Through just looking at these prior probabilites (The probability of a class being that class before we've seen the data) we can predict that we will most likely get a lot of E's in our predictions since it has such a high prior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "municipal-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_probabilites = {}\n",
    "b_probabilites = {}\n",
    "v_probabilites = {}\n",
    "e_probabilites = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-latitude",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "featured-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_word_sum = sum(a_counter.values())\n",
    "b_word_sum = sum(b_counter.values())\n",
    "v_word_sum = sum(v_counter.values())\n",
    "e_word_sum = sum(e_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adequate-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "overhead-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in all_unique:\n",
    "    if word not in a_counter.keys():\n",
    "        a_prob = 1\n",
    "    elif word in a_counter.keys():\n",
    "        a_prob = a_counter.get(word) + 1\n",
    "    a_probabilites[word] = a_prob / a_word_sum + sum_unique\n",
    "    \n",
    "    if word not in b_counter.keys():\n",
    "        b_prob = 1\n",
    "    elif word in b_counter.keys():\n",
    "        b_prob = b_counter.get(word) + 1\n",
    "    b_probabilites[word] = b_prob / b_word_sum + sum_unique\n",
    "    \n",
    "    if word not in v_counter.keys():\n",
    "        v_prob = 1\n",
    "    elif word in b_counter.keys():\n",
    "        v_prob = v_counter.get(word) + 1\n",
    "    v_probabilites[word] = v_prob / v_word_sum + sum_unique\n",
    "    \n",
    "        \n",
    "    if word not in e_counter.keys():\n",
    "        e_prob = 1\n",
    "    elif word in e_counter.keys():\n",
    "        e_prob = e_counter.get(word) + 1\n",
    "    e_probabilites[word] = e_prob / e_word_sum + sum_unique\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-brake",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "current-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E']\n"
     ]
    }
   ],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())\n",
    "a_word_sum = len(a_counter.values())\n",
    "b_word_sum = len(b_counter.values())\n",
    "v_word_sum = len(v_counter.values())\n",
    "e_word_sum = len(e_counter.values())\n",
    "predictions = []\n",
    "\n",
    "for i in test_data:\n",
    "    count_train = Counter(i[1].split())\n",
    "    #count_train = Counter(i[1])\n",
    "    class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "    for word in i[1].split():\n",
    "        a_prob = a_probabilites.get(word)\n",
    "        if (a_prob == None):\n",
    "            a_prob = 1 / a_word_sum + sum_unique\n",
    "        class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "        \n",
    "\n",
    "        b_prob = b_probabilites.get(word) \n",
    "        if (b_prob == None):\n",
    "            b_prob = 1 / b_word_sum + sum_unique\n",
    "        class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "        \n",
    "\n",
    "        v_prob = v_probabilites.get(word)\n",
    "        if (v_prob == None):\n",
    "            v_prob = 1 / v_word_sum + sum_unique\n",
    "        class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "        \n",
    "        e_prob = e_probabilites.get(word)\n",
    "        if (e_prob == None):\n",
    "            e_prob = 1 / e_word_sum + sum_unique\n",
    "        class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "    #print(class_prob)\n",
    "    class_prob[\"A\"] = class_prob[\"A\"] + math.log(a_probability)\n",
    "    class_prob[\"B\"] = class_prob[\"B\"] + math.log(b_probability)\n",
    "    class_prob[\"V\"] = class_prob[\"V\"] + math.log(v_probability)\n",
    "    class_prob[\"E\"] = class_prob[\"E\"] + math.log(e_probability)\n",
    "    predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "##print(all_unique)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-cancellation",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes\n",
    "For my extension of Naive Bayes I decided to add TF * IDF to discount words which would appear very frequently in the documents. If a word would appear in many documents in different classes then it won't really help us in our classification as that word most likely isn't related to the class and is hence pretty useless. With TF * IDF we apply a weighting to our words to prefer the rarer words and make their probability for a class higher than common words which will apear in every text, the texts with high frequency will recieve a weighting which reduces the probability of that word given a class which has significantly increased the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-convert",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atmospheric-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(data, index):\n",
    "    real_counter = Counter()\n",
    "    for doc in data:\n",
    "        a_set = set(doc[index].split())\n",
    "        real_counter = Counter(a_set) + real_counter\n",
    "    return real_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "employed-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32216\n"
     ]
    }
   ],
   "source": [
    "print(len(get_word_freq(data, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interracial-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_train(training_split_len, all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, a_counter, b_counter, e_counter, v_counter):\n",
    "    a_probabilites = {}\n",
    "    b_probabilites = {}\n",
    "    v_probabilites = {}\n",
    "    e_probabilites = {}\n",
    "    \n",
    "    for word in all_unique:\n",
    "\n",
    "        idf = math.log(training_split_len / word_freq_in_doc.get(word))\n",
    "\n",
    "        if word not in a_counter.keys():\n",
    "            a_prob = 1\n",
    "        elif word in a_counter.keys():\n",
    "            a_prob = a_counter.get(word) + 1\n",
    "        a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "        if a_prob == 0 or a_prob < 0:\n",
    "            a_prob = 0.00001\n",
    "        a_probabilites[word] = a_prob \n",
    "\n",
    "        if word not in b_counter.keys():\n",
    "            b_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            b_prob = b_counter.get(word) + 1\n",
    "        b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "        if b_prob == 0 or b_prob < 0:\n",
    "            b_prob = 0.00001\n",
    "        b_probabilites[word] = b_prob\n",
    "\n",
    "        if word not in v_counter.keys():\n",
    "            v_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            v_prob = v_counter.get(word) + 1\n",
    "        v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "        if v_prob == 0 or v_prob < 0:\n",
    "            v_prob = 0.00001\n",
    "        v_probabilites[word] = v_prob\n",
    "\n",
    "\n",
    "        if word not in e_counter.keys():\n",
    "            e_prob = 1\n",
    "        elif word in e_counter.keys():\n",
    "            e_prob = e_counter.get(word) + 1\n",
    "        e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "        if e_prob == 0 or e_prob < 0:\n",
    "            e_prob = 0.00001\n",
    "        e_probabilites[word] = e_prob\n",
    "        \n",
    "    return a_probabilites, b_probabilites, e_probabilites, v_probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "iraqi-fever",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "expensive-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_probabilites, b_probabilites, e_probabilites, v_probabilites = tf_idf_train(len(data),all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique,  a_counter, b_counter, e_counter, v_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-license",
   "metadata": {},
   "source": [
    "### TF * IDF Naive Bayes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bound-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'A', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'A', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'A', 'B', 'B', 'E', 'B', 'A', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'A', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'A', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'A', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'A', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'A', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'A', 'E', 'E', 'A', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'A', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'A', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'A', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'A', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'A', 'A', 'B', 'E', 'B', 'E', 'E', 'A', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'A', 'E', 'E', 'A', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'A', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'A', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'A', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'A', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'B', 'B', 'A', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'A', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'A', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'A', 'E']\n"
     ]
    }
   ],
   "source": [
    "def tf_idf_test(a_word_sum, b_word_sum, v_word_sum, e_word_sum,test_data, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, index):\n",
    "    predictions = []\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "        for word in i[index].split():\n",
    "            a_prob = a_probabilites.get(word)\n",
    "            if (a_prob == None):\n",
    "                a_prob = 1 / a_word_sum + sum_unique\n",
    "            class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "\n",
    "            b_prob = b_probabilites.get(word) \n",
    "            if (b_prob == None):\n",
    "                b_prob = 1 / b_word_sum + sum_unique\n",
    "            class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            v_prob = v_probabilites.get(word)\n",
    "            if (v_prob == None):\n",
    "                v_prob = 1 / v_word_sum + sum_unique\n",
    "            class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "\n",
    "            e_prob = e_probabilites.get(word)\n",
    "            if (e_prob == None):\n",
    "                e_prob = 1 / e_word_sum + sum_unique\n",
    "            class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        class_prob[\"A\"] =  math.log(a_probability) + class_prob[\"A\"]\n",
    "        class_prob[\"B\"] =  math.log(b_probability) + class_prob[\"B\"]\n",
    "        class_prob[\"V\"] =  math.log(v_probability) + class_prob[\"V\"]\n",
    "        class_prob[\"E\"] =  math.log(e_probability) + class_prob[\"E\"]\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    return predictions\n",
    "predictions = tf_idf_test(a_sum, b_sum, v_sum, e_sum,test_data, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 1)\n",
    "##print(all_unique)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-trust",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "To test how our algoirthm  would perform I chose to use cross validation as my tactic to get a significant result, although I wanted to use stratified cross validation but this would be really really computationally expensive due to the nature of how I created the algorithm. Stratified k-fold cross validation would ensure that we got a good distribution of classes in our testing and training set but due to time constraints, running such an algorithm would t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "intense-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92.75]\n",
      "[92.75, 91.75]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-644ec88d836a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mv_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mword_freq_in_doc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#    a_counter = Counter(dict(a_counter.most_common(1250)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-d9303da50402>\u001b[0m in \u001b[0;36mget_counts\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"E\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0me_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mword_freq\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"V\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__iadd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keep_positive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m_keep_positive\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_keep_positive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[1;34m'''Internal method to strip elements with a negative or zero count'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m         \u001b[0mnonpositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    803\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnonpositive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_keep_positive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[1;34m'''Internal method to strip elements with a negative or zero count'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m         \u001b[0mnonpositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    803\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnonpositive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "rand_split = cross_validation_split(data, k)\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    for i in range(k):\n",
    "        test_split = rand_split[i]\n",
    "        training_split = rand_split.copy()\n",
    "        training_split.pop(i)\n",
    "\n",
    "        training_split = [item for sublist in training_split for item in sublist]\n",
    "        text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "        a_counter = Counter()\n",
    "        e_counter = Counter()\n",
    "        b_counter = Counter()\n",
    "        v_counter = Counter()\n",
    "\n",
    "        word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "    #    a_counter = Counter(dict(a_counter.most_common(1250)))\n",
    "    #    b_counter = Counter(dict(b_counter.most_common(1250)))\n",
    "    #    e_counter = Counter(dict(e_counter.most_common(1250)))\n",
    "    #    v_counter = Counter(dict(v_counter.most_common(1250)))\n",
    "\n",
    "        all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "        sum_unique = len(all_unique.keys())\n",
    "\n",
    "        a_sum = sum(target_counter.values())\n",
    "        a_probability = target_counter[\"A\"] / a_sum\n",
    "        b_probability = target_counter[\"B\"] / a_sum\n",
    "        v_probability = target_counter[\"V\"] / a_sum\n",
    "        e_probability = target_counter[\"E\"] / a_sum\n",
    "\n",
    "\n",
    "        a_sum = len(a_counter.values())\n",
    "        b_sum = len(b_counter.values())\n",
    "        v_sum = len(v_counter.values())\n",
    "        e_sum = len(e_counter.values())\n",
    "\n",
    "        a_probabilites, b_probabilites, e_probabilites, v_probabilites = tf_idf_train(len(training_split), all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, a_counter, b_counter, e_counter, v_counter)\n",
    "        predictions = tf_idf_test(a_sum, b_sum, v_sum, e_sum,test_split, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 2)\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy_metric(test_split, predictions))\n",
    "        print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sound-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.575\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-sensitivity",
   "metadata": {},
   "source": [
    "### Accuracy with TF*IDF\n",
    "after a 10-fold cross validation which was run 10 times I got an average accuracy of 91.704166666667 which shows us that our model performed much better after applying a wieghting to the frequencies of the words overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-facing",
   "metadata": {},
   "source": [
    "### Complement TF*IDF Naive Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "decreased-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_train(training_split_len, all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique,a_counter, b_counter, e_counter, v_counter ):\n",
    "    a_probabilites = {}\n",
    "    b_probabilites = {}\n",
    "    v_probabilites = {}\n",
    "    e_probabilites = {}\n",
    "    \n",
    "    for word in all_unique:\n",
    "\n",
    "        idf = math.log(training_split_len / word_freq_in_doc.get(word))\n",
    "\n",
    "        if word not in a_counter.keys():\n",
    "            a_prob = 1\n",
    "        elif word in a_counter.keys():\n",
    "            a_prob = a_counter.get(word) + 1\n",
    "        a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "        if a_prob == 0 or a_prob < 0:\n",
    "            a_prob = 0.00001\n",
    "        a_probabilites[word] = math.log(a_prob)\n",
    "\n",
    "        if word not in b_counter.keys():\n",
    "            b_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            b_prob = b_counter.get(word) + 1\n",
    "        b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "        if b_prob == 0 or b_prob < 0:\n",
    "            b_prob = 0.00001\n",
    "        b_probabilites[word] = math.log(b_prob)\n",
    "\n",
    "        if word not in v_counter.keys():\n",
    "            v_prob = 1\n",
    "        elif word in b_counter.keys():\n",
    "            v_prob = v_counter.get(word) + 1\n",
    "        v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "        if v_prob == 0 or v_prob < 0:\n",
    "            v_prob = 0.00001\n",
    "        v_probabilites[word] = math.log(v_prob)\n",
    "\n",
    "\n",
    "        if word not in e_counter.keys():\n",
    "            e_prob = 1\n",
    "        elif word in e_counter.keys():\n",
    "            e_prob = e_counter.get(word) + 1\n",
    "        e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "        if e_prob == 0 or e_prob < 0:\n",
    "            e_prob = 0.00001\n",
    "        e_probabilites[word] = math.log(e_prob)\n",
    "        \n",
    "    return a_probabilites, b_probabilites, e_probabilites, v_probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "governmental-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_test(word_freq_in_doc,a_sum, b_sum, v_sum, e_sum, training_split_len, test_data, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, index):\n",
    "    predictions = []\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "        for word in i[index].split():\n",
    "            \n",
    "            if word in word_freq_in_doc:\n",
    "                word_in_doc = word_freq_in_doc[word]\n",
    "            else:\n",
    "                word_in_doc = 0\n",
    "    #        print(word_in_doc)\n",
    "            #Calculate the idf \n",
    "            if word_in_doc != 0:\n",
    "                idf = math.log(training_split_len / word_in_doc)\n",
    "            else:\n",
    "                idf = 0\n",
    "            \n",
    "            \n",
    "            a_prob = a_probabilites.get(word)\n",
    "            if (a_prob == None):\n",
    "                a_prob = (math.log(1)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "                \n",
    "            if a_prob == 0 or a_prob < 0:\n",
    "                a_prob = 0.000001\n",
    "            else:\n",
    "                a_prob = math.log(a_prob) \n",
    "            class_prob[\"A\"] = a_prob * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "\n",
    "            b_prob = b_probabilites.get(word) \n",
    "            if (b_prob == None):\n",
    "                b_prob = (math.log(1)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "                \n",
    "            if b_prob == 0 or b_prob < 0:\n",
    "                b_prob = 0.00001\n",
    "            else:\n",
    "                b_prob = math.log(b_prob)\n",
    "            class_prob[\"B\"] =  b_prob * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            v_prob = v_probabilites.get(word)\n",
    "            if (v_prob == None):\n",
    "                v_prob = (math.log(1)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "            if v_prob == 0 or v_prob < 0:\n",
    "                v_prob = 0.00001\n",
    "            else:\n",
    "                v_prob = math.log(v_prob)\n",
    "            class_prob[\"V\"] = v_prob * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "\n",
    "            e_prob = e_probabilites.get(word)\n",
    "            if (e_prob == None):\n",
    "                e_prob = (math.log(1)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "                \n",
    "            if e_prob == 0 or e_prob < 0:\n",
    "                e_prob = 0.00001\n",
    "            else:\n",
    "                e_prob = math.log(b_prob)\n",
    "            class_prob[\"E\"] = e_prob * count_train[word] + class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        class_prob[\"A\"] =  math.log(a_probability) - class_prob[\"A\"]\n",
    "        class_prob[\"B\"] =  math.log(b_probability) - class_prob[\"B\"]\n",
    "        class_prob[\"V\"] =  math.log(v_probability) - class_prob[\"V\"]\n",
    "        class_prob[\"E\"] =  math.log(e_probability) - class_prob[\"E\"]\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    return predictions\n",
    "##print(all_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "utility-crack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.25]\n",
      "[48.25, 53.0]\n",
      "[48.25, 53.0, 54.0]\n",
      "[48.25, 53.0, 54.0, 52.75]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001, 56.25]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001, 56.25, 52.5]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001, 56.25, 52.5, 53.25]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001, 56.25, 52.5, 53.25, 56.25]\n",
      "[48.25, 53.0, 54.0, 52.75, 56.00000000000001, 56.25, 52.5, 53.25, 56.25, 53.75]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "rand_split = cross_validation_split(data, k)\n",
    "accuracies = []\n",
    "for i in range(k):\n",
    "    test_split = rand_split[i]\n",
    "    training_split = rand_split.copy()\n",
    "    training_split.pop(i)\n",
    "   \n",
    "    training_split = [item for sublist in training_split for item in sublist]\n",
    "    text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "    a_counter = Counter()\n",
    "    e_counter = Counter()\n",
    "    b_counter = Counter()\n",
    "    v_counter = Counter()\n",
    "    \n",
    "    word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "    all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "    sum_unique = len(all_unique.keys())\n",
    "    \n",
    "    a_sum = sum(target_counter.values())\n",
    "    a_probability = target_counter[\"A\"] / a_sum\n",
    "    b_probability = target_counter[\"B\"] / a_sum\n",
    "    v_probability = target_counter[\"V\"] / a_sum\n",
    "    e_probability = target_counter[\"E\"] / a_sum\n",
    "    \n",
    "    \n",
    "    # Complement naive bayes so get all words in other classes\n",
    "    not_a = e_counter + b_counter + v_counter\n",
    "    not_b = a_counter + e_counter + v_counter\n",
    "    not_e = a_counter + v_counter + b_counter\n",
    "    not_v = a_counter + e_counter + b_counter\n",
    "    #Get the sum of all counts in a class. Total word count for each class\n",
    "    a_sum = sum(not_a.values())\n",
    "    b_sum = sum(not_b.values())\n",
    "    e_sum = sum(not_e.values())\n",
    "    v_sum = sum(not_v.values())\n",
    "    \n",
    "    a_probabilites, b_probabilites, e_probabilites, v_probabilites = complement_train(len(training_split), all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, not_a, not_b, not_e, not_v)\n",
    "    predictions = complement_test(word_freq_in_doc,a_sum, b_sum, v_sum, e_sum,len(training_split),test_split, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 2)\n",
    "    \n",
    "    \n",
    "    accuracies.append(accuracy_metric(test_split, predictions))\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "gross-rebound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91.25]\n",
      "[91.25, 94.75]\n",
      "[91.25, 94.75, 89.5]\n",
      "[91.25, 94.75, 89.5, 94.5]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75, 92.25]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75, 92.25, 95.25]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75, 92.25, 95.25, 92.5]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75, 92.25, 95.25, 92.5, 91.75]\n",
      "[91.25, 94.75, 89.5, 94.5, 92.75, 92.25, 95.25, 92.5, 91.75, 92.75]\n",
      "[93.25]\n",
      "[93.25, 90.5]\n",
      "[93.25, 90.5, 91.75]\n",
      "[93.25, 90.5, 91.75, 94.5]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0, 91.25]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0, 91.25, 94.75]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0, 91.25, 94.75, 93.5]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0, 91.25, 94.75, 93.5, 91.75]\n",
      "[93.25, 90.5, 91.75, 94.5, 93.0, 91.25, 94.75, 93.5, 91.75, 94.5]\n",
      "[92.0]\n",
      "[92.0, 91.75]\n",
      "[92.0, 91.75, 94.0]\n",
      "[92.0, 91.75, 94.0, 93.25]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75, 93.75]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75, 93.75, 92.5]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75, 93.75, 92.5, 93.75]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75, 93.75, 92.5, 93.75, 90.75]\n",
      "[92.0, 91.75, 94.0, 93.25, 94.75, 93.75, 92.5, 93.75, 90.75, 94.25]\n",
      "[93.0]\n",
      "[93.0, 95.0]\n",
      "[93.0, 95.0, 91.0]\n",
      "[93.0, 95.0, 91.0, 91.5]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25, 95.0]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25, 95.0, 94.25]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25, 95.0, 94.25, 90.75]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25, 95.0, 94.25, 90.75, 94.5]\n",
      "[93.0, 95.0, 91.0, 91.5, 92.25, 95.0, 94.25, 90.75, 94.5, 94.0]\n",
      "[92.0]\n",
      "[92.0, 94.0]\n",
      "[92.0, 94.0, 95.5]\n",
      "[92.0, 94.0, 95.5, 92.0]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5, 94.5]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5, 94.5, 93.5]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5, 94.5, 93.5, 91.0]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5, 94.5, 93.5, 91.0, 89.5]\n",
      "[92.0, 94.0, 95.5, 92.0, 94.5, 94.5, 93.5, 91.0, 89.5, 94.5]\n",
      "[94.75]\n",
      "[94.75, 94.5]\n",
      "[94.75, 94.5, 92.5]\n",
      "[94.75, 94.5, 92.5, 91.5]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75, 95.75]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75, 95.75, 93.25]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75, 95.75, 93.25, 91.75]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75, 95.75, 93.25, 91.75, 93.75]\n",
      "[94.75, 94.5, 92.5, 91.5, 93.75, 95.75, 93.25, 91.75, 93.75, 90.75]\n",
      "[92.25]\n",
      "[92.25, 93.5]\n",
      "[92.25, 93.5, 94.75]\n",
      "[92.25, 93.5, 94.75, 94.5]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5, 92.5]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5, 92.5, 92.25]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5, 92.5, 92.25, 92.25]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5, 92.5, 92.25, 92.25, 91.75]\n",
      "[92.25, 93.5, 94.75, 94.5, 93.5, 92.5, 92.25, 92.25, 91.75, 91.5]\n",
      "[92.25]\n",
      "[92.25, 94.25]\n",
      "[92.25, 94.25, 93.0]\n",
      "[92.25, 94.25, 93.0, 96.5]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0, 92.75]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0, 92.75, 91.5]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0, 92.75, 91.5, 91.75]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0, 92.75, 91.5, 91.75, 92.25]\n",
      "[92.25, 94.25, 93.0, 96.5, 91.0, 92.75, 91.5, 91.75, 92.25, 94.5]\n",
      "[94.5]\n",
      "[94.5, 92.5]\n",
      "[94.5, 92.5, 91.75]\n",
      "[94.5, 92.5, 91.75, 94.0]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75, 92.0]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75, 92.0, 92.25]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75, 92.0, 92.25, 92.5]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75, 92.0, 92.25, 92.5, 95.25]\n",
      "[94.5, 92.5, 91.75, 94.0, 90.75, 92.0, 92.25, 92.5, 95.25, 93.75]\n",
      "[94.75]\n",
      "[94.75, 93.0]\n",
      "[94.75, 93.0, 94.5]\n",
      "[94.75, 93.0, 94.5, 93.0]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5, 93.5]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5, 93.5, 91.5]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5, 93.5, 91.5, 92.5]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5, 93.5, 91.5, 92.5, 92.0]\n",
      "[94.75, 93.0, 94.5, 93.0, 90.5, 93.5, 91.5, 92.5, 92.0, 94.25]\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = []\n",
    "for j in range(10):\n",
    "    k = 10\n",
    "    rand_split = cross_validation_split(data, k)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        test_split = rand_split[i]\n",
    "        training_split = rand_split.copy()\n",
    "        training_split.pop(i)\n",
    "\n",
    "        training_split = [item for sublist in training_split for item in sublist]\n",
    "        text_in_class = {'E':{}, 'B':{}, 'A': {}, 'V':{}}\n",
    "        a_counter = Counter()\n",
    "        e_counter = Counter()\n",
    "        b_counter = Counter()\n",
    "        v_counter = Counter()\n",
    "\n",
    "        word_freq_in_doc,target_counter, a_counter, b_counter, e_counter, v_counter = get_counts(training_split)\n",
    "\n",
    "        a_sum = sum(target_counter.values())\n",
    "        a_probability = target_counter[\"A\"] / a_sum\n",
    "        b_probability = target_counter[\"B\"] / a_sum\n",
    "        v_probability = target_counter[\"V\"] / a_sum\n",
    "        e_probability = target_counter[\"E\"] / a_sum\n",
    "\n",
    "        training_len = len(training_split)\n",
    "\n",
    "        predictions = complement_idf(a_counter, b_counter, v_counter, e_counter, a_probability, b_probability, v_probability, e_probability, test_split, training_len, 2)\n",
    "\n",
    "\n",
    "    #    a_probabilites, b_probabilites, e_probabilites, v_probabilites = complement_train(len(training_split), all_unique, word_freq_in_doc,a_sum, b_sum, e_sum, v_sum, sum_unique, not_a, not_b, not_e, not_v)\n",
    "     #   predictions = complement_test(word_freq_in_doc,a_sum, b_sum, v_sum, e_sum,len(training_split),test_split, a_probabilites, b_probabilites, e_probabilites, v_probabilites, a_probability, b_probability, v_probability, e_probability, 2)\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy_metric(test_split, predictions))\n",
    "    print(accuracies)\n",
    "    overall_accuracy.append(statistics.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "composite-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.985\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(overall_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-sleeping",
   "metadata": {},
   "source": [
    "### Smoothing out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optimum-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "all_unique = Counter(dict(all_unique.most_common(5000)))\n",
    "sum_unique = len(all_unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "expressed-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(a_counter.keys()):\n",
    "    if i not in all_unique.keys():\n",
    "        a_counter.pop(i)\n",
    "for i in list(b_counter.keys()):\n",
    "    if i not in all_unique.keys():\n",
    "        b_counter.pop(i)\n",
    "for i in list(v_counter.keys()):\n",
    "    if i not in all_unique.keys():\n",
    "        v_counter.pop(i)\n",
    "for i in list(e_counter.keys()):\n",
    "    if i not in all_unique.keys():\n",
    "        e_counter.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "following-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_counter = Counter(dict(a_counter.most_common(5000)))\n",
    "b_counter = Counter(dict(b_counter.most_common(5000)))\n",
    "e_counter = Counter(dict(e_counter.most_common(5000)))\n",
    "v_counter = Counter(dict(v_counter.most_common(5000)))\n",
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "coordinated-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum = sum(target_counter.values())\n",
    "a_probability = target_counter[\"A\"] / a_sum\n",
    "b_probability = target_counter[\"B\"] / a_sum\n",
    "v_probability = target_counter[\"V\"] / a_sum\n",
    "e_probability = target_counter[\"E\"] / a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "extensive-sessions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prior = 0.032\n",
      "B prior = 0.4005\n",
      "V prior = 0.0315\n",
      "E prior = 0.536\n"
     ]
    }
   ],
   "source": [
    "print(\"A prior = {0}\".format(a_probability))\n",
    "print(\"B prior = {0}\".format(b_probability))\n",
    "print(\"V prior = {0}\".format(v_probability))\n",
    "print(\"E prior = {0}\".format(e_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-floor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weekly-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())\n",
    "a_word_sum = len(a_counter.keys())\n",
    "b_word_sum = len(b_counter.keys())\n",
    "v_word_sum = len(v_counter.keys())\n",
    "e_word_sum = len(e_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "genuine-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "all_unique = a_counter + b_counter + v_counter + e_counter\n",
    "sum_unique = len(all_unique.keys())\n",
    "a_word_sum = len(a_counter.values())\n",
    "b_word_sum = len(b_counter.values())\n",
    "v_word_sum = len(v_counter.values())\n",
    "e_word_sum = len(e_counter.values())\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for i in test_data:\n",
    "    count_train = Counter(i[1].split())\n",
    "    #count_train = Counter(i[1])\n",
    "    class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "    for word in i[1].split():\n",
    "        a_prob = a_probabilites.get(word)\n",
    "        if (a_prob == None):\n",
    "            a_prob = 1 / a_word_sum + sum_unique\n",
    "        class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "        \n",
    "\n",
    "        b_prob = b_probabilites.get(word) \n",
    "        if (b_prob == None):\n",
    "            b_prob = 1 / b_word_sum + sum_unique\n",
    "        class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "        \n",
    "\n",
    "        v_prob = v_probabilites.get(word)\n",
    "        if (v_prob == None):\n",
    "            v_prob = 1 / v_word_sum + sum_unique\n",
    "        class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "        \n",
    "        e_prob = e_probabilites.get(word)\n",
    "        if (e_prob == None):\n",
    "            e_prob = 1 / e_word_sum + sum_unique\n",
    "        class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "    #print(class_prob)\n",
    "    class_prob[\"A\"] = class_prob[\"A\"] + math.log(a_probability)\n",
    "    class_prob[\"B\"] = class_prob[\"B\"] + math.log(b_probability)\n",
    "    class_prob[\"V\"] = class_prob[\"V\"] + math.log(v_probability)\n",
    "    class_prob[\"E\"] = class_prob[\"E\"] + math.log(e_probability)\n",
    "    predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "##print(all_unique)\n",
    "print(predictions)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dominican-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram(s, n):\n",
    "    \n",
    "    ngrams = [ngram for ngram in s.split(\" \") if ngram != \"\"]\n",
    "    \n",
    "    ngrams = zip(*[ngrams[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "coated-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V']\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "not_a = e_counter + b_counter + v_counter\n",
    "not_b = a_counter + e_counter + v_counter\n",
    "not_e = a_counter + v_counter + b_counter\n",
    "not_v = a_counter + e_counter + b_counter\n",
    "\n",
    "a_sum = sum(not_a.values())\n",
    "b_sum = sum(not_b.values())\n",
    "v_sum = sum(not_v.values())\n",
    "e_sum = sum(not_e.values())\n",
    "\n",
    "for i in test_data:\n",
    "    #count_train = Counter(i[1].split())\n",
    "    count_train = Counter(i[1])\n",
    "    class_prob = {\"A\": 0, \"B\": 0, \"V\": 0, \"E\": 0}\n",
    "    for word in i[1]:\n",
    "        if word not in not_a.keys():\n",
    "            a_prob = 1\n",
    "        elif word in not_a.keys():\n",
    "            a_prob = not_a.get(word) + 1\n",
    "        a_prob = a_prob /  a_sum + len(not_a.keys())\n",
    "        class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "        \n",
    "        if word not in not_b.keys():\n",
    "            b_prob = 1\n",
    "        elif word in not_b.keys():\n",
    "            b_prob = not_b.get(word) + 1\n",
    "        b_prob = b_prob / b_sum + len(not_b.keys())\n",
    "        class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "        \n",
    "\n",
    "        if word not in not_v.keys():\n",
    "            v_prob = 1\n",
    "        elif word in not_v.keys():\n",
    "            v_prob = not_v.get(word) + 1\n",
    "        v_prob = v_prob / v_sum + len(not_v.keys())\n",
    "        class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "        if word not in not_e.keys():\n",
    "            e_prob = 1\n",
    "        elif word in not_e.keys():\n",
    "            e_prob = not_e.get(word) + 1\n",
    "        e_prob = e_prob / e_sum + len(not_v.keys())\n",
    "        class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "    class_prob[\"A\"] = 1/class_prob[\"A\"] + math.log(a_probability)\n",
    "    class_prob[\"B\"] = 1/class_prob[\"B\"] + math.log(b_probability)\n",
    "    class_prob[\"V\"] = 1/class_prob[\"V\"] + math.log(v_probability)\n",
    "    class_prob[\"E\"] = 1/class_prob[\"E\"] + math.log(e_probability)\n",
    "    predictions.append(min(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sitting-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set all unique words to 1 as it will appear once in a document\n",
    "word_freq_in_doc = {}\n",
    "for word in all_unique.keys():\n",
    "    word_freq_in_doc[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "circular-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the frequency of word in separate documents\n",
    "for word in all_unique.keys():\n",
    "    for doc in data:\n",
    "        if word in doc[2]:\n",
    "            word_freq_in_doc[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "opposed-cricket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "closed-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'A', 'B', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'A', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'A', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'A', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'A', 'A', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'A', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E']\n"
     ]
    }
   ],
   "source": [
    "def complement_idf(a_counter, b_counter, v_counter, e_counter, a_probability, b_probability, v_probability, e_probability, test_data, training_len, index):\n",
    "    predictions = []\n",
    "    # Complement naive bayes so get all words in other classes\n",
    "    not_a = e_counter + b_counter + v_counter\n",
    "    not_b = a_counter + e_counter + v_counter\n",
    "    not_e = a_counter + v_counter + b_counter\n",
    "    not_v = a_counter + e_counter + b_counter\n",
    "    \n",
    "    sum_unique = a_counter + b_counter + v_counter + e_counter\n",
    "    sum_unique = len(sum_unique.keys())\n",
    "\n",
    "    #Get the sum of all counts in a class. Total word count for each class\n",
    "    a_sum = sum(not_a.values())\n",
    "    b_sum = sum(not_b.values())\n",
    "    e_sum = sum(not_e.values())\n",
    "    v_sum = sum(not_v.values())\n",
    "\n",
    "    #Iterate through the csv\n",
    "    for i in test_data:\n",
    "        count_train = Counter(i[index].split())\n",
    "        # For every document, do a word count\n",
    "        #count_train = Counter(i[1])\n",
    "        class_prob = {\"A\": 1, \"B\": 1, \"V\": 1, \"E\": 1}\n",
    "        # Iterate through each word in a document\n",
    "        for word in i[index].split():\n",
    "            # Get the frequency of a word in all training documents\n",
    "            if word in word_freq_in_doc:\n",
    "                word_in_doc = word_freq_in_doc[word]\n",
    "            else:\n",
    "                word_in_doc = 0\n",
    "    #        print(word_in_doc)\n",
    "            #Calculate the idf \n",
    "            if word_in_doc != 0:\n",
    "                idf = math.log(training_len / word_in_doc)\n",
    "            else:\n",
    "                idf = 0\n",
    "            if word not in not_a.keys():\n",
    "                a_prob = 1\n",
    "            elif word in not_a.keys():\n",
    "                a_prob = not_a.get(word) + 1\n",
    "            a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "            if a_prob == 0 or a_prob < 0:\n",
    "                a_prob = 0.00001\n",
    "           # print(a_prob)\n",
    "            class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "\n",
    "            if word not in not_b.keys():\n",
    "                b_prob = 1\n",
    "            elif word in not_b.keys():\n",
    "                b_prob = not_b.get(word) + 1\n",
    "           # print(\"b_Probability \" + str(b_prob))\n",
    "            #print(\"idf \" + str(idf))\n",
    "            b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "            if b_prob == 0 or b_prob < 0:\n",
    "                b_prob = 0.00001\n",
    "            class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "\n",
    "\n",
    "            if word not in not_v.keys():\n",
    "                v_prob = 1\n",
    "            elif word in not_v.keys():\n",
    "                v_prob = not_v.get(word) + 1\n",
    "            v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "            if v_prob == 0 or v_prob <0:\n",
    "                v_prob = 0.00001\n",
    "            class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "\n",
    "            if word not in not_e.keys():\n",
    "                e_prob = 1\n",
    "            elif word in not_e.keys():\n",
    "                e_prob = not_e.get(word) + 1\n",
    "            e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "            if e_prob == 0 or e_prob < 0:\n",
    "                e_prob = 0.00001\n",
    "            class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        class_prob[\"A\"] =  math.log(a_probability) - class_prob[\"A\"]\n",
    "        class_prob[\"B\"] =  math.log(b_probability) - class_prob[\"B\"]\n",
    "        class_prob[\"V\"] =  math.log(v_probability) - class_prob[\"V\"]\n",
    "        class_prob[\"E\"] =  math.log(e_probability) - class_prob[\"E\"]\n",
    "        #print(class_prob)\n",
    "        predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "    return predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "hybrid-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'V', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'V', 'A', 'A', 'A', 'A', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'V', 'V', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'A', 'V', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'V', 'V']\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "# Complement naive bayes so get all words in other classes\n",
    "not_a = e_counter + b_counter + v_counter\n",
    "not_b = a_counter + e_counter + v_counter\n",
    "not_e = a_counter + v_counter + b_counter\n",
    "not_v = a_counter + e_counter + b_counter\n",
    "#Get the sum of all counts in a class. Total word count for each class\n",
    "a_sum = sum(not_a.values())\n",
    "b_sum = sum(not_b.values())\n",
    "e_sum = sum(not_e.values())\n",
    "v_sum = sum(not_v.values())\n",
    "\n",
    "#Iterate through the csv\n",
    "for i in test_data:\n",
    "    count_train = Counter(i[1].split())\n",
    "    # For every document, do a word count\n",
    "    #count_train = Counter(i[1])\n",
    "    class_prob = {\"A\": 1, \"B\": 1, \"V\": 1, \"E\": 1}\n",
    "    # Iterate through each word in a document\n",
    "    for word in i[1].split():\n",
    "        # Get the frequency of a word in all training documents\n",
    "        if word in word_freq_in_doc:\n",
    "            word_in_doc = word_freq_in_doc[word]\n",
    "        else:\n",
    "            word_in_doc = 0\n",
    "#        print(word_in_doc)\n",
    "        #Calculate the idf \n",
    "        if word_in_doc != 0:\n",
    "            idf = math.log(4000 / word_in_doc)\n",
    "        else:\n",
    "            idf = 0\n",
    "        if word not in not_a.keys():\n",
    "            a_prob = 1\n",
    "        elif word in not_a.keys():\n",
    "            a_prob = not_a.get(word) + 1\n",
    "        a_prob = (math.log(a_prob)*idf) / ((math.log(a_sum)*idf) + sum_unique)\n",
    "        if a_prob == 0 or a_prob < 0:\n",
    "            a_prob = 0.00001\n",
    "       # print(a_prob)\n",
    "        class_prob[\"A\"] = math.log(a_prob) * count_train[word] + class_prob[\"A\"]\n",
    "        class_prob[\"A\"] = class_prob[\"A\"] * count_train[word]\n",
    "    \n",
    "        \n",
    "        if word not in not_b.keys():\n",
    "            b_prob = 1\n",
    "        elif word in not_b.keys():\n",
    "            b_prob = not_b.get(word) + 1\n",
    "       # print(\"b_Probability \" + str(b_prob))\n",
    "        #print(\"idf \" + str(idf))\n",
    "        b_prob = (math.log(b_prob)*idf) / ((math.log(b_sum)*idf) + sum_unique)\n",
    "        if b_prob == 0 or b_prob < 0:\n",
    "            b_prob = 0.00001\n",
    "        class_prob[\"B\"] =  math.log(b_prob) * count_train[word] + class_prob[\"B\"]\n",
    "        class_prob[\"B\"] = class_prob[\"B\"] * count_train[word]\n",
    "\n",
    "        if word not in not_v.keys():\n",
    "            v_prob = 1\n",
    "        elif word in not_v.keys():\n",
    "            v_prob = not_v.get(word) + 1\n",
    "        v_prob = (math.log(v_prob)*idf) / ((math.log(v_sum)*idf) + sum_unique)\n",
    "        if v_prob == 0 or v_prob <0:\n",
    "            v_prob = 0.00001\n",
    "        class_prob[\"V\"] = math.log(v_prob) * count_train[word] + class_prob[\"V\"]\n",
    "        class_prob[\"V\"] = class_prob[\"V\"] * count_train[word]\n",
    "        \n",
    "        if word not in not_e.keys():\n",
    "            e_prob = 1\n",
    "        elif word in not_e.keys():\n",
    "            e_prob = not_e.get(word) + 1\n",
    "        e_prob = (math.log(e_prob)*idf) / ((math.log(e_sum)*idf) + sum_unique)\n",
    "        if e_prob == 0 or e_prob < 0:\n",
    "            e_prob = 0.00001\n",
    "        class_prob[\"E\"] = math.log(e_prob) * count_train[word] + class_prob[\"E\"]\n",
    "        class_prob[\"E\"] = class_prob[\"E\"] * count_train[word]\n",
    "        \n",
    "        #print(sum(class_prob.values()))\n",
    "    class_prob[\"A\"] =  math.log(a_probability) - class_prob[\"A\"]\n",
    "    class_prob[\"B\"] =  math.log(b_probability) - class_prob[\"B\"]\n",
    "    class_prob[\"V\"] =  math.log(v_probability) - class_prob[\"V\"]\n",
    "    class_prob[\"E\"] =  math.log(e_probability) - class_prob[\"E\"]\n",
    "    #print(class_prob)\n",
    "    predictions.append(max(class_prob.items(), key=operator.itemgetter(1))[0])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "textile-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', ['in a', 'a previous', 'previous work', 'work all', 'all three', 'three components', 'components of', 'of comamonas', 'comamonas testosteroni', 'testosteroni b-356', 'b-356 biphenyl', 'biphenyl bphchlorobiphenyls', 'bphchlorobiphenyls pcbs', 'pcbs dioxygenase', 'dioxygenase dox', 'dox have', 'have been', 'been purified', 'purified and', 'and characterized', 'characterized they', 'they include', 'include an', 'an iron-sulphur', 'iron-sulphur protein', 'protein ispbph', 'ispbph which', 'which is', 'is the', 'the terminal', 'terminal oxygenase', 'oxygenase composed', 'composed of', 'of two', 'two subunits', 'subunits encoded', 'encoded by', 'by bpha', 'bpha and', 'and bphe', 'bphe a', 'a ferredoxin', 'ferredoxin ferbph', 'ferbph encoded', 'encoded by', 'by bphf', 'bphf and', 'and a', 'a reductase', 'reductase redbph', 'redbph encoded', 'encoded by', 'by bphg', 'bphg bphg', 'bphg is', 'is not', 'not located', 'located in', 'in the', 'the neighbourhood', 'neighbourhood of', 'of bphaef', 'bphaef in', 'in b-356', 'b-356 we', 'we are', 'are reporting', 'reporting the', 'the cloning', 'cloning of', 'of b-356-bphg', 'b-356-bphg and', 'and the', 'the sequencing', 'sequencing of', 'of b-356-bph', 'b-356-bph dox', 'dox genes', 'genes comparative', 'comparative analysis', 'analysis of', 'of the', 'the genes', 'genes provided', 'provided genetic', 'genetic evidence', 'evidence showing', 'showing that', 'that two', 'two bph', 'bph dox', 'dox lineages', 'lineages have', 'have emerged', 'emerged in', 'in gram-negative', 'gram-negative bacteria', 'bacteria the', 'the main', 'main features', 'features of', 'of the', 'the lineage', 'lineage that', 'that includes', 'includes b-356', 'b-356 are', 'are the', 'the location', 'location of', 'of bphg', 'bphg outside', 'outside the', 'the bph', 'bph gene', 'gene cluster', 'cluster and', 'and the', 'the structure', 'structure of', 'of redbph', 'redbph which', 'which is', 'is very', 'very distinct', 'distinct from', 'from all', 'all other', 'other aryl', 'aryl dioxygenase-reductases']]\n"
     ]
    }
   ],
   "source": [
    "with open('tst.csv', newline='') as csvfile:\n",
    "    test_data = list(csv.reader(csvfile))\n",
    "    for i in test_data:\n",
    "        n_gram = create_ngram(i[1], 2)\n",
    "        i[1] = n_gram\n",
    "print(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "moderate-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'B', ['the 4', '4 202', '202 353', '353 bp', 'bp genome', 'genome of', 'of the', 'the alkaliphilic', 'alkaliphilic bacterium', 'bacterium bacillus', 'bacillus halodurans', 'halodurans c-125', 'c-125 contains', 'contains 4066', '4066 predicted', 'predicted protein', 'protein coding', 'coding sequences', 'sequences cdss', 'cdss 2141', '2141 527', '527 of', 'of which', 'which have', 'have functional', 'functional assignments', 'assignments 1182', '1182 29', '29 of', 'of which', 'which are', 'are conserved', 'conserved cdss', 'cdss with', 'with unknown', 'unknown function', 'function and', 'and 743', '743 18', '18 3', '3 of', 'of which', 'which have', 'have no', 'no match', 'match to', 'to any', 'any protein', 'protein database', 'database among', 'among the', 'the total', 'total cdss', 'cdss 88', '88 match', 'match sequences', 'sequences of', 'of proteins', 'proteins found', 'found only', 'only in', 'in bacillus', 'bacillus subtilis', 'subtilis and', 'and 667', '667 are', 'are widely', 'widely conserved', 'conserved in', 'in comparison', 'comparison with', 'with the', 'the proteins', 'proteins of', 'of various', 'various organisms', 'organisms including', 'including bsubtilis', 'bsubtilis the', 'the b', 'b halodurans', 'halodurans genome', 'genome contains', 'contains 112', '112 transposase', 'transposase genes', 'genes indicating', 'indicating that', 'that transposases', 'transposases have', 'have played', 'played an', 'an important', 'important evolutionary', 'evolutionary role', 'role in', 'in horizontal', 'horizontal gene', 'gene transfer', 'transfer and', 'and also', 'also in', 'in internal', 'internal genetic', 'genetic rearrangement', 'rearrangement in', 'in the', 'the genome', 'genome strain', 'strain c-125', 'c-125 lacks', 'lacks some', 'some of', 'of the', 'the necessary', 'necessary genes', 'genes for', 'for competence', 'competence such', 'such as', 'as coms', 'coms srfa', 'srfa and', 'and rapc', 'rapc supporting', 'supporting the', 'the fact', 'fact that', 'that competence', 'competence has', 'has not', 'not been', 'been demonstrated', 'demonstrated experimentally', 'experimentally in', 'in c-125', 'c-125 there', 'there is', 'is no', 'no paralog', 'paralog of', 'of tupa', 'tupa encoding', 'encoding teichuronopeptide', 'teichuronopeptide which', 'which contributes', 'contributes to', 'to alkaliphily', 'alkaliphily in', 'in the', 'the c-125', 'c-125 genome', 'genome and', 'and an', 'an ortholog', 'ortholog of', 'of tupa', 'tupa cannot', 'cannot be', 'be found', 'found in', 'in the', 'the bsubtilis', 'bsubtilis genome', 'genome out', 'out of', 'of 11', '11 sigma', 'sigma factors', 'factors which', 'which belong', 'belong to', 'to the', 'the extracytoplasmic', 'extracytoplasmic function', 'function family', 'family 10', '10 are', 'are unique', 'unique to', 'to b', 'b halodurans', 'halodurans suggesting', 'suggesting that', 'that they', 'they may', 'may have', 'have a', 'a role', 'role in', 'in the', 'the special', 'special mechanism', 'mechanism of', 'of adaptation', 'adaptation to', 'to an', 'an alkaline', 'alkaline environment']]\n"
     ]
    }
   ],
   "source": [
    "with open('trg.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    for i in data:\n",
    "        n_gram = create_ngram(i[2], 2)\n",
    "        i[2] = n_gram\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fitted-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"class\"])\n",
    "    for i in range(len(predictions)):\n",
    "        writer.writerow([i+1, predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-saudi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
